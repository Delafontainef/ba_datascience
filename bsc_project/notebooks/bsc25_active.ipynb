{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc71ff8e-6261-42ad-a84d-9ac72bc62fa3",
   "metadata": {},
   "source": [
    "# Bsc25 - Active training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e31bf6c-32b0-4e7f-9ee5-7938c48aa422",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8779ae0-595b-4365-925f-837121fc7aef",
   "metadata": {},
   "source": [
    "The purpose of this work is to experiment with active learning.\n",
    "\n",
    "To do this, we:\n",
    "1. Develop a model (CRF) for a given dataset (spoken French).\n",
    "2. Test that model with passive (conventional) training.\n",
    "3. Test that model with active training.\n",
    "\n",
    "The dataset is the OFROM+ corpus containing ~2mn tokens (*words*) of spoken French. Tokens are grouped in IPUs (*inter-pausal units*) separated by pauses of >=0.5s. IPUs are grouped in *files* corresponding to the TextGrid files containing the transcriptions. \n",
    "\n",
    "The model is a simple CRF (*Conditional Random Field*) model that uses IPUs as *sequences*, with the *token* as sole factor. \n",
    "\n",
    "Testing means taking an initial subset (usually 1-10k tokens), training the model, then adding to that subset (another 1-10k tokens) and re-training, iteratively until a set limit or the original dataset is exhausted. At each step we retrieve an *accuracy score*, as well as a set of tokens of interest. The subset and additional data is selected by *file*, adding files until the token count is reached: this is because researchers would use files as their minimal unit for correction and sharing.\n",
    "\n",
    "Passive training selects the additional data at random. \n",
    "\n",
    "Active training follows an automated strategy based on a file's value and cost as well as the set of tokens from the previous step. The full formula is discussed in point (3).\n",
    "\n",
    "Results are provided in the form of charts showing the evolution of the accuracy score. Several iterations of testing allow for a confidence interval: the y-axis offers the number of iterations in parenthesis. While the set of tokens varies at each step, another mode of active training allows maintaining the same set: this lets us produce a graph showing the evolution of those tokens' confidence score throughout the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f104770-684e-433c-908d-ffeb5a74248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ofrom_train as tr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431498ed-8acb-4759-9a06-82f8bef1728c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Data and Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721ec723-b0df-4235-8a3f-46c497d96642",
   "metadata": {},
   "source": [
    "The OFROM+ corpus is a set of TextGrid files containing transcriptions of spoken French. Those transcriptions were annotated into PoS (*Part-of-Speech*, grammatical labels such as verb, noun, adjective, etc.) with the automatic annotation tool DisMo.\n",
    "\n",
    "As a result, the OFROM+ team also has a dictionary with, for each token, all of its possible tags. This dictionary was originally used for correction purposes and ignores false negatives (missing tags for a token) but allows us to find non-problematic tokens, that is, tokens with a single possible tag, as well as find tokens with grammatical tags (excluding purely lexical tokens: nouns, verbs, numeric...). This will be used in active training.\n",
    "\n",
    "We initially transformed the TextGrid files into a DataFrame with a *token* per row and (meta)data in columns: file, speaker, timestamps, PoS, lemma, confidence score, etc. This is the 'ofrom_alt.joblib' file. We further parse that list of *tokens* to group them into *sequences* and those sequences by *file*; and for each file we collect the number of occurrences for each token. This results in a list of files (a file being here a list of sequences) as well as a DataFrame with file information. (We also recreate the OFROM+ dictionary with tags per token, but the resulting 'pos' dictionary isn't used during testing.)\n",
    "\n",
    "At this step we already calculate the file's weight and cost, which we will discuss in point (3). \n",
    "\n",
    "The result is stored in 'ofrom_gen.joblib'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a4d5196-458b-4c9d-b16a-0665e9d4db7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr.regen(\"code/ofrom_alt.joblib\", \"code/ofrom_gen.joblib\")    # generate 'ofrom_gen.joblib'\n",
    "gen = tr._load_gen()                                            # loads the data, \n",
    "                                                                ## assumes 'code/ofrom_gen.joblib' as path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8eaa7d-38e2-4da2-bf08-6705b09b9cd0",
   "metadata": {},
   "source": [
    "A typical training dataset would be 100k tokens. Corrections would usually be by batches of 10k tokens (~5-10 files). Runs of 1k tokens are purely for technical testing.\n",
    "\n",
    "When training the model during our testing, we will actually cross-train, that is, split the data into batches (5 by default), keep one batch for testing (accuracy score and confidence scores) and train the model on the rest; we train as many models as there are batches and average the accuracy_scores. This is done to ensure all tokens receive a confidence score. We have hard-coded the process to train all models in parallel as threads for a given subset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11a0822-93fe-4f39-b936-ac11733fcee9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Passive training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183ddaad-2ade-4509-8619-80938df5f5c3",
   "metadata": {},
   "source": [
    "With passive training, we can immediately train our CRF model on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753b7009-92df-4f79-bca3-42439af85b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_model = gen.crf_passive()    # train a model on the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a7676e-a7fc-44b0-bbed-fc9a04c70263",
   "metadata": {},
   "source": [
    "But for comparison purposes, we iterate on subsets selected at random. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaf479a-3b50-46b4-a171-0f5a8df31f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.reset()\n",
    "tr.passive(gen, lim=10000, loop=10, nb_batches=5)    # one iteration with 10 steps of 10k tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd11953d-60ff-4361-aa94-5b79446ce81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.save_passive(it=10, f=\"json/passive_10k_10.json\") # add 10 iterations of 'passive' to the json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a3cad4-7b72-4ac0-b464-f10ee816a678",
   "metadata": {},
   "source": [
    "We can then observe the learning rate (accuracy_score / nb_tokens) on a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd33213d-f142-418e-b60d-10a7b660aa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.plot_acc(f=\"json/passive_10k_10.json\", lim=10000, \n",
    "            title=\"Passive training\")                # plots the iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a47b146-62a6-45bb-8d88-751a491a0b94",
   "metadata": {},
   "source": [
    "The 'plot_acc()' function directly saves the image using the title as name. Here are a couple plots.\n",
    "![passive_10k_10](img/passive_10k_10.png \"passive_1k_10\")\n",
    "![passive_1k_100](img/passive_1k_100.png \"passive_1k_10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33334514-0194-4720-8095-d5a5826e6d44",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Active training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84521a30-3c7e-435a-8139-2cb7a7917302",
   "metadata": {},
   "source": [
    "Active training requires a strategy to select the next files for the subset. Our current formula is:\n",
    "> ( (tok_coeff\\*token_weight) \\* (file_coeff\\*file_weight) ) / file_cost\n",
    "\n",
    "Where the coefficients (X_coeff) are manually set and fixed throughout the process. By default:\n",
    "- tok_coeff = 1.\n",
    "- file_coeff = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065508c6-676e-46c1-85a1-ac132881d166",
   "metadata": {},
   "source": [
    "The *token_weight* formula is:\n",
    "> sum( token_occurrences_in_file \\* (1-token_confidence_score) ) / nb_tokens\n",
    "\n",
    "That is, the average of confidence scores multiplied by the amount of occurrences in the file: the more of it and the more uncertain the better. \n",
    "\n",
    "As for how the set of tokens is selected at each step, the formula is:\n",
    "> log10( token_occurrences_in_subset ) \\* (1-token_confidence_score)\n",
    "\n",
    "Picking the lowest confidence score usually ends up selecting scarce occurrences; we therefore take the number of occurrences into account. This also tends to eliminate tokens that may have exhausted their occurrences in the dataset. The logarithmic scale avoids the highest count dominating by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bff5b1b-ee60-4280-99dd-45eff5fbb19f",
   "metadata": {},
   "source": [
    "The *file_weight*, while eliminated by default (due to file_coeff == 0.), is calculated by summing the file's tokens (occurrences) with at least one grammatical tag and dividing that sum by the total number of tokens (occurrences). That percentage represents a file's potential value, as one way among others to seek variety and relevance for training & research.\n",
    "\n",
    "The *file_cost* is the number of *problematic* tokens (occurrences) in a file, that is, the number of occurrences with more than one possible tag and therefore required to correct. This isn't exactly an ENUA (Expected Number of User Actions) as we don't give each occurrence a probability of being corrected, but is still a good estimate of the amount of work expected in a manual correction.\n",
    "\n",
    "Those two values are fixed throughout the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb08bfa-1630-47e3-8e25-ac28057c2d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.reset()\n",
    "tr.active_variable(gen, lim=10000, loop=10, \n",
    "                   nb_batches=5, nb_toks=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366f632c-2662-4059-ae9b-9b8d1fb31cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.save_active_v(it=10, f=\"json/active_10k_10.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a388c19-29f7-4ebc-aa66-9d13b512af6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.plot_acc(f=\"json/active_10k_10.json\", lim=10000, \n",
    "            title=\"Active training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b63526-5452-4072-8bab-3c707ee83277",
   "metadata": {},
   "source": [
    "Again, 'plot_acc()' directly saves the graphs. \n",
    "![active_1k_10](img/active_1k_10.png \"active_1k_10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f082662-1158-4fd5-b323-77549c02f417",
   "metadata": {},
   "source": [
    "It is possible that users already know what tokens of interest they want to track. This *fixed active training* is active training where the set of tokens does not vary at each step: only the confidence scores (and number of occurrences) are updated. \n",
    "\n",
    "We can use the 'active_fixed()' function for that purpose. It can take an additional 'g_toks' parameter with a list of tuples (token, confidence_score). If that parameter is omitted, it will select an initial subset at random, then fix its set of tokens from that subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303f20e9-84fd-4c17-a2c5-eb49e7a3ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.reset()\n",
    "tr.active_fixed(gen, lim=10000, loop=10, nb_batches=5, nb_toks=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90dd972-d348-4fd8-b201-2f571ecffd44",
   "metadata": {},
   "source": [
    "No function has been set to save iterations in a json. The resulting graph is saved manually.\n",
    "![active_fixed](img/active_fixed.png \"active_fixed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b928a547-da35-45e0-8e57-c6c0d819dc11",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0e9051-ecfe-4029-a75a-981cad8f5a8f",
   "metadata": {},
   "source": [
    "First, regarding accuracy scores:\n",
    "- We can expect a 1k training set to provide an accuracy score of ~0.87.\n",
    "- At 10k tokens, it should be around ~0.92.\n",
    "- At 100k tokens, it should be around ~0.94.\n",
    "\n",
    "Our CRF model could not be made simpler (although we did not discuss here how some tokens/symbols were removed, such as truncations, shorter pauses or inaudible speech...). We conclude that a PoS annotator should not do worse than 0.94 and that value should be considered the floor. (The DisMo annotation tool has an accuracy score of ~0.98.)\n",
    "\n",
    "Second, regarding learning rates:\n",
    "- Passive training follows a nice, logarithmic curve. It plateaus around ~0.94-95.\n",
    "- Active training doesn't look more efficient.\n",
    "\n",
    "This goes against our expectation, as the purpose of active learning is to get a better accuracy score with fewer data. We have yet to formulate hypotheses to explain those observations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
