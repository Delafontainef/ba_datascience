{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd1649c1-d7e5-4808-80ac-e7d6f9fddefe",
   "metadata": {},
   "source": [
    "# Bsc25 - CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952aae14-91e0-43c4-bfbc-ed91e9ee7887",
   "metadata": {},
   "source": [
    "## Setting up a CRF pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f73df65-2338-4253-9241-ce05ed91a989",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73888e52-1a15-4c51-a32b-b4fc8eb3c27d",
   "metadata": {},
   "source": [
    "We turn our dataset 'ofrom_alt.joblib' into *sequences*.\n",
    "\n",
    "The dataset is derived from the OFROM+ database of spoken French. The joblib object is a Pandas DataFrame with one row per *token* (~word).\n",
    "\n",
    "A *sequence* is an IPU for Intra-Pausal Unit, meaning a set of *tokens* between two (silent) pauses. The rest of this paragraph is a discussion for linguists. Relevant pauses are based on their duration, with the old DisMo model using a 0.5s (second) threshold, whereas our IPU threshold is set at 0.3s. Linguistically, 0.3s is the duration at which pauses start being perceived, while ~0.6-0.8s is when they start getting considered as proper boundaries. We have chosen the lower threshold based on Fribourg's pragma-syntax for sequences as close as possible to *clauses*.\n",
    "\n",
    "When building a sequence, some *tokens* are discarded. Those are:\n",
    "- shorter pauses (<0.3s)\n",
    "- reserved symbols (anonymized or inintelligible parts, third-party locutor, etc.)\n",
    "- truncations (*tokens* interrupted before completion)\n",
    "\n",
    "While the choice of excluding those cases is not trivial, we made the bet that they would add noise more than anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8059fab3-55f1-4668-ada5-3ae20de7de3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ofrom_crf # requires scikit-learn & sklearn_crfsuite\n",
    "import ofrom_pos # requires joblib, zipfile, networkx, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ffc4e53-3322-484f-bafd-935c2ace0b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = ofrom_pos.load_allsequs(lim=100000) # only take 10'000 first lines for demonstration\n",
    "X_tr, X_te, y_tr, y_te = ofrom_crf.train_test_split(X, y, train_size=0.8) # actually from scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad959c9-1191-45c7-93fb-3835253c6cb5",
   "metadata": {},
   "source": [
    "We then train a CRF (Conditional Random Fields) model on those sequences using the dedicated *sklearn_crfsuite* (more precisely its CRF class). \n",
    "\n",
    "- 'X' is a list of *sequences*, with each sequence being a list of dictionaries, each dictionary containing the feature for its related *token*. In our case, 'load_allsequs()' gave only the *token* string itself as feature, meaning our model has only 1 feature.\n",
    "- 'y' is a list of *sequences*, with each sequence being a list of strings representing the 'pos' (PoS standing for Part-of-Speech, the *token*'s morpho-syntactic / grammatical category).\n",
    "\n",
    "The hyperparameters 'c1' and 'c2' (which we do not yet understand) have been based on a preliminary work from fall 2024 and should be revised when possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30c92755-e52e-487b-9c99-1988c30f2398",
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = ofrom_crf.train(X_tr, y_tr, c1=0.22, c2=0.03, max_iterations=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d924ef-c430-4d80-b89e-8917587d685f",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6df6bd6-9f6b-48f6-a552-0888e01cf4d6",
   "metadata": {},
   "source": [
    "We can use that model on a single sequence and retrieve the confidence score for each *token*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fe69a034-9bcd-479f-a4b3-a1a486a03675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CON:coo              1.00\tet\t CON:coo\n",
      "PRP                  1.00\tpour\t PRP\n",
      "PRO:per:ton          1.00\tmoi\t PRO:per:ton\n",
      "VER:inf              1.00\tfaire\t VER:inf\n",
      "DET:def              1.00\tla\t DET:def\n",
      "NOM:com              0.97\tcuisine\t NOM:com\n",
      "ADJ                  0.44\tcuisiner\t VER:inf\n",
      "PRP                  1.00\tà\t PRP\n",
      "DET:ind              0.93\tdes\t DET:ind\n",
      "PRP                  0.99\tà\t PRP\n",
      "DET:ind              0.98\tdes\t DET:ind\n",
      "NOM:com              1.00\tpersonnes\t NOM:com\n",
      "CON:sub              0.63\tque\t CON:sub\n",
      "PRO:per:sjt          1.00\tje\t PRO:per:sjt\n",
      "VER:pres             0.87\tconnais\t VER:pres\n",
      "CON:coo              1.00\tou\t CON:coo\n",
      "ADV:neg              1.00\tpas\t ADV:neg\n",
      "CON:sub              0.87\tque\t CON:sub\n",
      "PRO:per:sjt          1.00\tje\t PRO:per:sjt\n",
      "PRP                  0.97\tdans\t PRP\n",
      "PRP                  1.00\tdans\t PRP\n",
      "DET:def              0.99\tles\t DET:def\n",
      "NUM:crd:det          0.54\tdeux\t NUM:crd:nom\n"
     ]
    }
   ],
   "source": [
    "nx, ny = X_te[8], y_te[8]                # pick a sequence\n",
    "l_res = ofrom_crf.predict_one(crf, nx)   # predict it\n",
    "for a, res in enumerate(l_res):          # print in a somewhat clean way...\n",
    "    pos, conf = res['pos'], res['confidence']\n",
    "    print(f\"{pos:<20} {conf:.02f}\\t{nx[a]['token']}\\t {ny[a]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152be80f-b043-41d3-ad84-a1b1668c0801",
   "metadata": {},
   "source": [
    "We also have the ability to iterate over sequences. *load_allsequs* is actually built over that generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4696845-d54a-42b0-922a-7b7e14860154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('qui', 'PRO:rel'), ('veut', 'VER:pres'), ('aller', 'VER:inf'), ('en', 'PRP'), ('Roumanie', 'NOM:com'), ('elle', 'PRO:per:sjt'), ('toujours', 'ADV'), ('non', 'ADV:neg'), ('Bucarest', 'VER:ppas')]\n",
      "[('parce', 'CON:sub'), (\"qu'\", 'CON:sub'), ('y', 'PRO:per:obji'), ('avait', 'VER:impf'), ('pas', 'ADV:neg'), ('ah', 'ITJ'), ('non', 'ADV:neg'), ('parce', 'CON:sub'), (\"qu'\", 'CON:sub')]\n"
     ]
    }
   ],
   "source": [
    "for nx, ny in ofrom_pos.iter_sequ(s=200000, lim=20, ch_prep=True):\n",
    "    l_res = ofrom_crf.predict_one(crf, nx)\n",
    "    print([(nx[a]['token'], res['pos']) for a, res in enumerate(l_res)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f5bda9-bd77-43d5-bdba-d92796b0aa59",
   "metadata": {},
   "source": [
    "And naturally we can predict over an entire set of sequences, which will be used for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c0bcbf-976a-4da6-ac21-0ad6ded28bd3",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff669e44-2c7a-4bd3-9d77-a605f666e010",
   "metadata": {},
   "source": [
    "We have so far limited our testing to a cross-validation score using our test subset 'X/y_te'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa0290aa-db4d-4ea7-a966-d3885cc6199d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9201842124550519\n"
     ]
    }
   ],
   "source": [
    "res = ofrom_crf.np.mean(ofrom_crf.cross_test(crf, X_te, y_te, cv=10)) # defaults cv=5, n_jobs=-1\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7dbbaa-2524-4088-9017-1643639bc30c",
   "metadata": {},
   "source": [
    "## Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463c0cf2-beae-48f1-8cd4-17f0a5874e96",
   "metadata": {},
   "source": [
    "This was thought as a demonstration of the pipeline using a CRF model. A previous run with only 10,000 lines resulted in a cross-validation score of 0.86. \n",
    "\n",
    "1. The only way to make the model simpler would be to simplify the tagset by only retaining the main category (the first three letters, such as *PRO* or *ADV*).\n",
    "2. As such, a score of 0.92 may be considered as a floor. In fact, when testing CRF models back in fall 2024, we obtained a score of 0.95 (with a simplified tagset).\n",
    "3. Steps to improve the model would be to add features and revise its hyperparameters.\n",
    "4. Steps to \"improve\" the pipeline may include breaking it into two layers, one CRF for the main 'pos' category and another for the sub-categories. Using a dictionary to handle *tokens* with a single possible tag does not seem relevant anymore.\n",
    "\n",
    "The actual priority now, with a score of 0.92 being sufficient, would be to use that pipeline for active training. \n",
    "\n",
    "But first, we would like to take time to simulate and acclimate ourselves with the Markov Decision Process, as well as learn about the theory surrounding the CRF model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
